# ===== Frontend IDs (remove hardcoded IDs) =====
NEXT_PUBLIC_AGENT_ID=research-twin-local
NEXT_PUBLIC_RAG_ID=default

# ===== Optional API auth =====
# If set, all /api routes require this token via Authorization: Bearer <token>
API_AUTH_TOKEN=change-me
# For this prototype UI, send the same token from browser fetches.
NEXT_PUBLIC_API_AUTH_TOKEN=change-me
# Optional basic auth alternative
# API_BASIC_USER=admin
# API_BASIC_PASS=change-me
# Optional: allow health endpoint without auth
HEALTH_SKIP_AUTH=false

# ===== Rate limiting =====
RATE_LIMIT_WINDOW_MS=60000
RATE_LIMIT_MAX=120
RATE_LIMIT_AGENT_MAX=240
RATE_LIMIT_UPLOAD_MAX=60
RATE_LIMIT_RAG_MAX=90
RATE_LIMIT_SCHEDULER_MAX=90
RATE_LIMIT_HEALTH_MAX=120
RATE_LIMIT_BENCHMARK_MAX=30

# ===== Chat backend =====
# Supported values: ollama | nvidia
CHAT_PROVIDER=ollama

# ===== NVIDIA NIM / NVIDIA API (for CHAT_PROVIDER=nvidia) =====
# Use API key from NVIDIA Build / API catalog
NVIDIA_API_KEY=change-me
# OpenAI-compatible API base
NVIDIA_API_BASE_URL=https://integrate.api.nvidia.com/v1
# Optional: if you have a full endpoint URL, set this instead
# NVIDIA_CHAT_URL=https://integrate.api.nvidia.com/v1/chat/completions
NVIDIA_CHAT_MODEL=nvidia/nemotron-nano-12b-v2-vl
NVIDIA_TEMPERATURE=0.4
NVIDIA_TOP_P=0.95
# Optional benchmark defaults (used by /api/model-benchmark)
BENCHMARK_TEMPERATURE=0.2
BENCHMARK_TOP_P=0.9

# ===== Self-hosted LLM stack (Ollama) =====
OLLAMA_BASE_URL=http://127.0.0.1:11434
OLLAMA_CHAT_MODEL=llama3.1:8b
OLLAMA_EMBEDDING_MODEL=nomic-embed-text
OLLAMA_TEMPERATURE=0.4

# ===== Local RAG =====
DEFAULT_RAG_ID=default
RAG_TOP_K=5
RAG_CHUNK_SIZE=900
RAG_CHUNK_OVERLAP=150

# ===== Agent/Scheduler =====
DEFAULT_USER_ID=local-user
AGENT_TASK_TTL_MS=900000

# ===== Storage =====
# Defaults to ./data if not set
# DATA_DIR=/app/data
